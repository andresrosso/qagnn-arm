{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "try:\n",
    "    from transformers import (ConstantLRSchedule, WarmupLinearSchedule, WarmupConstantSchedule)\n",
    "except:\n",
    "    from transformers import get_constant_schedule, get_constant_schedule_with_warmup,  get_linear_schedule_with_warmup\n",
    "\n",
    "from modeling.modeling_qagnn import *\n",
    "from utils.optimization_utils import OPTIMIZER_CLASSES\n",
    "from utils.parser_utils import *\n",
    "import utils\n",
    "from collections import defaultdict, OrderedDict\n",
    "import numpy as np\n",
    "\n",
    "import socket, os, subprocess, datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all tests are passed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([44,  1, 16, 62, 78,  9, 51, 26,  7, 71, 90, 57, 65, 52, 48, 78, 89, 58,\n",
       "        87, 58, 26, 90, 35, 29,  7, 46, 89, 82, 45, 34, 39, 85,  4, 53, 79, 39,\n",
       "        82, 64, 47, 70, 66, 76, 74, 57, 56, 36, 31, 88, 74, 75, 76, 14, 20, 88,\n",
       "        39, 57, 58, 95, 16,  0, 12, 62, 26, 90, 61, 27, 48, 77, 33, 29])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import modeling.modeling_encoder as enc\n",
    "\n",
    "encoder = enc.TextEncoder('lstm', vocab_size=100, emb_size=100, hidden_size=200, num_layers=4)\n",
    "input_ids = torch.randint(0, 100, (30, 70))\n",
    "lenghts = torch.randint(1, 70, (30,))\n",
    "outputs = encoder(input_ids, lenghts)\n",
    "assert outputs[0].size() == (30, 200)\n",
    "assert len(outputs[1]) == 4 + 1\n",
    "assert all([x.size() == (30, 70, 100 if l == 0 else 200) for l, x in enumerate(outputs[1])])\n",
    "print('all tests are passed')\n",
    "input_ids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling.modeling_encoder import TextEncoder, MODEL_NAME_TO_CLASS, MODEL_CLASS_TO_NAME\n",
    "from utils.data_utils import *\n",
    "from utils.layers import *\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model_name = 'bert'\n",
    "model_type = MODEL_CLASS_TO_NAME[model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bert-base-uncased',\n",
       " 'bert-large-uncased',\n",
       " 'bert-base-cased',\n",
       " 'bert-large-cased',\n",
       " 'bert-base-multilingual-uncased',\n",
       " 'bert-base-multilingual-cased',\n",
       " 'bert-base-chinese',\n",
       " 'bert-base-german-cased',\n",
       " 'bert-large-uncased-whole-word-masking',\n",
       " 'bert-large-cased-whole-word-masking',\n",
       " 'bert-large-uncased-whole-word-masking-finetuned-squad',\n",
       " 'bert-large-cased-whole-word-masking-finetuned-squad',\n",
       " 'bert-base-cased-finetuned-mrpc',\n",
       " 'bert-base-german-dbmdz-cased',\n",
       " 'bert-base-german-dbmdz-uncased',\n",
       " 'cl-tohoku/bert-base-japanese',\n",
       " 'cl-tohoku/bert-base-japanese-whole-word-masking',\n",
       " 'cl-tohoku/bert-base-japanese-char',\n",
       " 'cl-tohoku/bert-base-japanese-char-whole-word-masking',\n",
       " 'TurkuNLP/bert-base-finnish-cased-v1',\n",
       " 'TurkuNLP/bert-base-finnish-uncased-v1',\n",
       " 'wietsedv/bert-base-dutch-cased']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "encoder_config={}\n",
    "encoder = TextEncoder(model_name, **encoder_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MedQA params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = get_parser()\n",
    "dataset=\"medqa_usmle\"\n",
    "model='cambridgeltl/SapBERT-from-PubMedBERT-fulltext'\n",
    "ent_emb='ddb'\n",
    "saved_models = \"saved_models\"\n",
    "parser.set_defaults(mode=\"eval_detail\")\n",
    "parser.set_defaults(encoder=model)\n",
    "parser.set_defaults(ent_emb=ent_emb)\n",
    "parser.set_defaults(ent_emb_paths=[utils.parser_utils.EMB_PATHS['ddb']])\n",
    "parser.set_defaults(load_model_path=\"saved_models/medqa_usmle_model_hf3.4.0.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/ddb/ent_emb.npy'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.parser_utils.EMB_PATHS['ddb']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--use_cache'], dest='use_cache', nargs='?', const=True, default=True, type=<function bool_flag at 0x7fa3b94b1ea0>, choices=None, required=False, help='use cached data to accelerate data loading', metavar=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.set_defaults(dataset=dataset)\n",
    "parser.set_defaults(save_model=True)\n",
    "parser.set_defaults(save_dir=saved_models)\n",
    "parser.set_defaults(train_adj=\"data/{}/graph/dev.graph.adj.pk\".format(dataset))\n",
    "parser.set_defaults(dev_adj=\"data/{}/graph/dev.graph.adj.pk\".format(dataset))\n",
    "parser.set_defaults(test_adj=\"data/{}/graph/test.graph.adj.pk\".format(dataset))\n",
    "parser.set_defaults(train_statements=\"data/{}/statement/dev.statement.jsonl\".format(dataset))\n",
    "parser.set_defaults(dev_statements=\"data/{}/statement/dev.statement.jsonl\".format(dataset))\n",
    "parser.set_defaults(test_statements=\"data/{}/statement/test.statement.jsonl\".format(dataset))\n",
    "parser.add_argument('-ebs', '--eval_batch_size', default=2, type=int)\n",
    "parser.add_argument('--subsample', default=1.0, type=float)\n",
    "parser.add_argument('--use_cache', default=True, type=bool_flag, nargs='?', const=True, help='use cached data to accelerate data loading')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(ent_emb='ddb', dataset='medqa_usmle', inhouse=True, inhouse_train_qids='data/csqa/inhouse_split_qids.txt', train_statements='data/medqa_usmle/statement/dev.statement.jsonl', dev_statements='data/medqa_usmle/statement/dev.statement.jsonl', test_statements='data/medqa_usmle/statement/test.statement.jsonl', max_seq_len=100, encoder='cambridgeltl/SapBERT-from-PubMedBERT-fulltext', encoder_layer=-1, encoder_lr=2e-05, loss='cross_entropy', optim='radam', lr_schedule='fixed', batch_size=32, warmup_steps=150, max_grad_norm=1.0, weight_decay=0.01, n_epochs=100, max_epochs_before_stop=10, log_interval=10, cuda=True, seed=0, debug=False, eval_batch_size=2, subsample=1.0, use_cache=True, ent_emb_paths=['data/ddb/ent_emb.npy'], mode='eval_detail', load_model_path='saved_models/medqa_usmle_model_hf3.4.0.pt', save_model=True, save_dir='saved_models', train_adj='data/medqa_usmle/graph/dev.graph.adj.pk', dev_adj='data/medqa_usmle/graph/dev.graph.adj.pk', test_adj='data/medqa_usmle/graph/test.graph.adj.pk')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args, _ = parser.parse_known_args()\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/ddb/ent_emb.npy']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.ent_emb_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate QAGNN\n",
    "Mainly from qagnn.py -> evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| num_concepts: 9958 |\n"
     ]
    }
   ],
   "source": [
    "assert args.load_model_path is not None\n",
    "cp_emb = [np.load(path) for path in args.ent_emb_paths]\n",
    "cp_emb = torch.tensor(np.concatenate(cp_emb, 1), dtype=torch.float)\n",
    "concept_num, concept_dim = cp_emb.size(0), cp_emb.size(1)\n",
    "print('| num_concepts: {} |'.format(concept_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/ddb/ent_emb.npy']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.ent_emb_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concept_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'saved_models/medqa_usmle_model_hf3.4.0.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model_path \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mload_model_path\n\u001b[0;32m----> 2\u001b[0m model_state_dict, old_args \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m LM_QAGNN(old_args, old_args\u001b[38;5;241m.\u001b[39mencoder, k\u001b[38;5;241m=\u001b[39mold_args\u001b[38;5;241m.\u001b[39mk, n_ntype\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, n_etype\u001b[38;5;241m=\u001b[39mold_args\u001b[38;5;241m.\u001b[39mnum_relation, n_concept\u001b[38;5;241m=\u001b[39mconcept_num,\n\u001b[1;32m      4\u001b[0m                            concept_dim\u001b[38;5;241m=\u001b[39mold_args\u001b[38;5;241m.\u001b[39mgnn_dim,\n\u001b[1;32m      5\u001b[0m                            concept_in_dim\u001b[38;5;241m=\u001b[39mconcept_dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m                            init_range\u001b[38;5;241m=\u001b[39mold_args\u001b[38;5;241m.\u001b[39minit_range,\n\u001b[1;32m     10\u001b[0m                            encoder_config\u001b[38;5;241m=\u001b[39m{})\n\u001b[1;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(model_state_dict)\n",
      "File \u001b[0;32m~/.conda/envs/haystack-py310/lib/python3.10/site-packages/torch/serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    789\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 791\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    793\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/.conda/envs/haystack-py310/lib/python3.10/site-packages/torch/serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 271\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/.conda/envs/haystack-py310/lib/python3.10/site-packages/torch/serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 252\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'saved_models/medqa_usmle_model_hf3.4.0.pt'"
     ]
    }
   ],
   "source": [
    "model_path = args.load_model_path\n",
    "model_state_dict, old_args = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "model = LM_QAGNN(old_args, old_args.encoder, k=old_args.k, n_ntype=4, n_etype=old_args.num_relation, n_concept=concept_num,\n",
    "                           concept_dim=old_args.gnn_dim,\n",
    "                           concept_in_dim=concept_dim,\n",
    "                           n_attention_head=old_args.att_head_num, fc_dim=old_args.fc_dim, n_fc_layer=old_args.fc_layer_num,\n",
    "                           p_emb=old_args.dropouti, p_gnn=old_args.dropoutg, p_fc=old_args.dropoutf,\n",
    "                           pretrained_concept_emb=cp_emb, freeze_ent_emb=old_args.freeze_ent_emb,\n",
    "                           init_range=old_args.init_range,\n",
    "                           encoder_config={})\n",
    "model.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(att_head_num=2, batch_size=128, cuda=True, dataset='medqa_usmle', debug=False, decoder_lr=0.001, dev_adj='data/medqa_usmle/graph/dev.graph.adj.pk', dev_statements='data/medqa_usmle/statement/dev.statement.jsonl', dropoutf=0.2, dropoutg=0.2, dropouti=0.2, encoder='cambridgeltl/SapBERT-from-PubMedBERT-fulltext', encoder_layer=-1, encoder_lr=5e-05, ent_emb=['ddb'], ent_emb_paths=['data/ddb/ent_emb.npy'], eval_batch_size=2, fc_dim=200, fc_layer_num=0, fp16=True, freeze_ent_emb=True, gnn_dim=200, inhouse=False, inhouse_train_qids='data/medqa_usmle/inhouse_split_qids.txt', init_range=0.02, k=5, load_model_path=None, log_interval=10, loss='cross_entropy', lr_schedule='fixed', max_epochs_before_stop=10, max_grad_norm=1.0, max_node_num=200, max_seq_len=512, mini_batch_size=2, mode='train', n_epochs=15, num_relation=34, optim='radam', refreeze_epoch=10000, save_dir='saved_models/medqa_usmle/enc-sapbert__k5__gnndim200__bs128__seed1__20211106_120518', save_model=True, seed=1, simple=False, subsample=1.0, test_adj='data/medqa_usmle/graph/test.graph.adj.pk', test_statements='data/medqa_usmle/statement/test.statement.jsonl', train_adj='data/medqa_usmle/graph/train.graph.adj.pk', train_statements='data/medqa_usmle/statement/train.statement.jsonl', unfreeze_epoch=0, use_cache=True, warmup_steps=150, weight_decay=0.01)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() >= 2 and args.cuda:\n",
    "    device0 = torch.device(\"cuda:0\")\n",
    "    device1 = torch.device(\"cuda:1\")\n",
    "elif torch.cuda.device_count() == 1 and args.cuda:\n",
    "    device0 = torch.device(\"cuda:0\")\n",
    "    device1 = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device0 = torch.device(\"cpu\")\n",
    "    device1 = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LM_QAGNN(\n",
       "  (encoder): TextEncoder(\n",
       "    (module): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): QAGNN(\n",
       "    (concept_emb): CustomizedEmbedding(\n",
       "      (emb): Embedding(9958, 768)\n",
       "      (cpt_transform): Linear(in_features=768, out_features=200, bias=True)\n",
       "      (activation): GELU()\n",
       "    )\n",
       "    (svec2nvec): Linear(in_features=768, out_features=200, bias=True)\n",
       "    (activation): GELU()\n",
       "    (gnn): QAGNN_Message_Passing(\n",
       "      (emb_node_type): Linear(in_features=4, out_features=100, bias=True)\n",
       "      (emb_score): Linear(in_features=100, out_features=100, bias=True)\n",
       "      (edge_encoder): Sequential(\n",
       "        (0): Linear(in_features=43, out_features=200, bias=True)\n",
       "        (1): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Linear(in_features=200, out_features=200, bias=True)\n",
       "      )\n",
       "      (gnn_layers): ModuleList(\n",
       "        (0): GATConvE(\n",
       "          (edge_encoder): Sequential(\n",
       "            (0): Linear(in_features=43, out_features=200, bias=True)\n",
       "            (1): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=200, out_features=200, bias=True)\n",
       "          )\n",
       "          (linear_key): Linear(in_features=600, out_features=200, bias=True)\n",
       "          (linear_msg): Linear(in_features=600, out_features=200, bias=True)\n",
       "          (linear_query): Linear(in_features=400, out_features=200, bias=True)\n",
       "          (mlp): Sequential(\n",
       "            (0): Linear(in_features=200, out_features=200, bias=True)\n",
       "            (1): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=200, out_features=200, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (1): GATConvE(\n",
       "          (edge_encoder): Sequential(\n",
       "            (0): Linear(in_features=43, out_features=200, bias=True)\n",
       "            (1): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=200, out_features=200, bias=True)\n",
       "          )\n",
       "          (linear_key): Linear(in_features=600, out_features=200, bias=True)\n",
       "          (linear_msg): Linear(in_features=600, out_features=200, bias=True)\n",
       "          (linear_query): Linear(in_features=400, out_features=200, bias=True)\n",
       "          (mlp): Sequential(\n",
       "            (0): Linear(in_features=200, out_features=200, bias=True)\n",
       "            (1): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=200, out_features=200, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (2): GATConvE(\n",
       "          (edge_encoder): Sequential(\n",
       "            (0): Linear(in_features=43, out_features=200, bias=True)\n",
       "            (1): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=200, out_features=200, bias=True)\n",
       "          )\n",
       "          (linear_key): Linear(in_features=600, out_features=200, bias=True)\n",
       "          (linear_msg): Linear(in_features=600, out_features=200, bias=True)\n",
       "          (linear_query): Linear(in_features=400, out_features=200, bias=True)\n",
       "          (mlp): Sequential(\n",
       "            (0): Linear(in_features=200, out_features=200, bias=True)\n",
       "            (1): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=200, out_features=200, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (3): GATConvE(\n",
       "          (edge_encoder): Sequential(\n",
       "            (0): Linear(in_features=43, out_features=200, bias=True)\n",
       "            (1): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=200, out_features=200, bias=True)\n",
       "          )\n",
       "          (linear_key): Linear(in_features=600, out_features=200, bias=True)\n",
       "          (linear_msg): Linear(in_features=600, out_features=200, bias=True)\n",
       "          (linear_query): Linear(in_features=400, out_features=200, bias=True)\n",
       "          (mlp): Sequential(\n",
       "            (0): Linear(in_features=200, out_features=200, bias=True)\n",
       "            (1): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=200, out_features=200, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (4): GATConvE(\n",
       "          (edge_encoder): Sequential(\n",
       "            (0): Linear(in_features=43, out_features=200, bias=True)\n",
       "            (1): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=200, out_features=200, bias=True)\n",
       "          )\n",
       "          (linear_key): Linear(in_features=600, out_features=200, bias=True)\n",
       "          (linear_msg): Linear(in_features=600, out_features=200, bias=True)\n",
       "          (linear_query): Linear(in_features=400, out_features=200, bias=True)\n",
       "          (mlp): Sequential(\n",
       "            (0): Linear(in_features=200, out_features=200, bias=True)\n",
       "            (1): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=200, out_features=200, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (Vh): Linear(in_features=200, out_features=200, bias=True)\n",
       "      (Vx): Linear(in_features=200, out_features=200, bias=True)\n",
       "      (activation): GELU()\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (pooler): MultiheadAttPoolLayer(\n",
       "      (w_qs): Linear(in_features=768, out_features=200, bias=True)\n",
       "      (w_ks): Linear(in_features=200, out_features=200, bias=True)\n",
       "      (w_vs): Linear(in_features=200, out_features=200, bias=True)\n",
       "      (attention): MatrixVectorScaledDotProductAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (softmax): Softmax(dim=1)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (fc): MLP(\n",
       "      (layers): Sequential(\n",
       "        (0-Linear): Linear(in_features=1168, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (dropout_e): Dropout(p=0.2, inplace=False)\n",
       "    (dropout_fc): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.to(device0)\n",
    "model.decoder.to(device1)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inhouse? True\n",
      "args.train_statements data/medqa_usmle/statement/dev.statement.jsonl\n",
      "args.dev_statements data/medqa_usmle/statement/dev.statement.jsonl\n",
      "args.test_statements data/medqa_usmle/statement/test.statement.jsonl\n",
      "args.train_adj data/medqa_usmle/graph/dev.graph.adj.pk\n",
      "args.dev_adj data/medqa_usmle/graph/dev.graph.adj.pk\n",
      "args.test_adj data/medqa_usmle/graph/test.graph.adj.pk\n"
     ]
    }
   ],
   "source": [
    "statement_dic = {}\n",
    "for statement_path in (args.train_statements, args.dev_statements, args.test_statements):\n",
    "    statement_dic.update(load_statement_dict(statement_path))\n",
    "\n",
    "use_contextualized = 'lm' in old_args.ent_emb\n",
    "\n",
    "print ('inhouse?', args.inhouse)\n",
    "\n",
    "print ('args.train_statements', args.train_statements)\n",
    "print ('args.dev_statements', args.dev_statements)\n",
    "print ('args.test_statements', args.test_statements)\n",
    "print ('args.train_adj', args.train_adj)\n",
    "print ('args.dev_adj', args.dev_adj)\n",
    "print ('args.test_adj', args.test_adj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from qagnn/qagnn.py\n",
    "def evaluate_accuracy(eval_set, model):\n",
    "    n_samples, n_correct = 0, 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for qids, labels, *input_data in tqdm(eval_set):\n",
    "            logits, _ = model(*input_data)\n",
    "            n_correct += (logits.argmax(1) == labels).sum().item()\n",
    "            n_samples += labels.size(0)\n",
    "    return n_correct / n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_statement_path data/medqa_usmle/statement/dev.statement.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1272/1272 [00:12<00:00, 100.09it/s]\n",
      "100%|██████████| 1272/1272 [00:12<00:00, 101.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_choice 4\n",
      "| ori_adj_len: mu 25.20 sigma 28.18 | adj_len: 26.20 | prune_rate： 0.00 | qc_num: 4.34 | ac_num: 1.07 |\n",
      "| ori_adj_len: mu 25.20 sigma 28.18 | adj_len: 26.20 | prune_rate： 0.00 | qc_num: 4.34 | ac_num: 1.07 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1273/1273 [00:12<00:00, 98.73it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ori_adj_len: mu 26.09 sigma 28.17 | adj_len: 27.06 | prune_rate： 0.00 | qc_num: 4.61 | ac_num: 1.06 |\n"
     ]
    }
   ],
   "source": [
    "dataset = LM_QAGNN_DataLoader(args, args.train_statements, args.train_adj,\n",
    "                                       args.dev_statements, args.dev_adj,\n",
    "                                       args.test_statements, args.test_adj,\n",
    "                                       batch_size=args.batch_size, eval_batch_size=args.eval_batch_size,\n",
    "                                       device=(device0, device1),\n",
    "                                       model_name=old_args.encoder,\n",
    "                                       max_node_num=old_args.max_node_num, max_seq_length=old_args.max_seq_len,\n",
    "                                       is_inhouse=args.inhouse, inhouse_train_qids_path=args.inhouse_train_qids,\n",
    "                                       subsample=args.subsample, use_cache=args.use_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Mode Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 636/636 [02:52<00:00,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_acc  0.3789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "save_test_preds = args.save_model\n",
    "dev_acc = evaluate_accuracy(dataset.dev(), model)\n",
    "print('dev_acc {:7.4f}'.format(dev_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 216/636 [00:57<01:52,  3.73it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-c913a420c287>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mqids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                 \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcept_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetail\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m                 \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#[bsize, ]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0mpreds_ranked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#[bsize, n_choices]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/qagnn/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git_repos/qagnn/modeling/modeling_qagnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, layer_id, cache_output, detail, *inputs)\u001b[0m\n\u001b[1;32m    232\u001b[0m                                     \u001b[0mconcept_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                                     \u001b[0mnode_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                                     emb_data=None, cache_output=cache_output)\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdetail\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/qagnn/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git_repos/qagnn/modeling/modeling_qagnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sent_vecs, concept_ids, node_type_ids, node_scores, adj_lengths, adj, emb_data, cache_output)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mgnn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgnn_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mZ_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgnn_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;31m#(batch_size, dim_node)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/qagnn/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git_repos/qagnn/modeling/modeling_qagnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, H, A, node_type, node_score, cache_output)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m#Embed type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_one_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_ntype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_n_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_ntype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mnode_type_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb_node_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#[batch_size, n_node, dim/2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git_repos/qagnn/modeling/modeling_qagnn.py\u001b[0m in \u001b[0;36mmake_one_hot\u001b[0;34m(labels, C)\u001b[0m\n\u001b[1;32m    362\u001b[0m     '''\n\u001b[1;32m    363\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m     \u001b[0mone_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if not save_test_preds:\n",
    "    test_acc = evaluate_accuracy(dataset.test(), model) if args.test_statements else 0.0\n",
    "else:\n",
    "    eval_set = dataset.test()\n",
    "    total_acc = []\n",
    "    count = 0\n",
    "    dt = datetime.datetime.today().strftime('%Y%m%d%H%M%S')\n",
    "    preds_path = os.path.join(args.save_dir, 'test_preds_{}.csv'.format(dt))\n",
    "    with open(preds_path, 'w') as f_preds:\n",
    "        with torch.no_grad():\n",
    "            for qids, labels, *input_data in tqdm(eval_set):\n",
    "                count += 1\n",
    "                logits, _, concept_ids, node_type_ids, edge_index, edge_type = model(*input_data, detail=True)\n",
    "                predictions = logits.argmax(1) #[bsize, ]\n",
    "                preds_ranked = (-logits).argsort(1) #[bsize, n_choices]\n",
    "                for i, (qid, label, pred, _preds_ranked, cids, ntype, edges, etype) in enumerate(zip(qids, labels, predictions, preds_ranked, concept_ids, node_type_ids, edge_index, edge_type)):\n",
    "                    acc = int(pred.item()==label.item())\n",
    "                    print ('{},{}'.format(qid, chr(ord('A') + pred.item())), file=f_preds)\n",
    "                    f_preds.flush()\n",
    "                    total_acc.append(acc)\n",
    "    test_acc = float(sum(total_acc))/len(total_acc)\n",
    "\n",
    "    print('-' * 71)\n",
    "    print('test_acc {:7.4f}'.format(test_acc))\n",
    "    print('-' * 71)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Mode Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train-00000', 'train-00001'] tensor([2, 0], device='cuda:0')\n",
      "tensor([[[   0,  553,  821,  ...,    1,    1,    1],\n",
      "         [   0,  553,  821,  ...,    1,    1,    1],\n",
      "         [   0,  553,  821,  ...,    1,    1,    1],\n",
      "         [   0,  553,  821,  ...,    1,    1,    1]],\n",
      "\n",
      "        [[   0,   57, 2089,  ...,    1,    1,    1],\n",
      "         [   0,   57, 2089,  ...,    1,    1,    1],\n",
      "         [   0,   57, 2089,  ...,    1,    1,    1],\n",
      "         [   0,   57, 2089,  ...,    1,    1,    1]]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eval_set = dataset.test()\n",
    "subsample = list(eval_set)[0:1]#random.sample(list(eval_set), 2)\n",
    "total_acc = []\n",
    "count = 0\n",
    "dt = datetime.datetime.today().strftime('%Y%m%d%H%M%S')\n",
    "preds_path = os.path.join(args.save_dir, 'test_preds_{}.csv'.format(dt))\n",
    "with open(preds_path, 'w') as f_preds:\n",
    "    with torch.no_grad():\n",
    "        for qids, labels, *input_data in tqdm(subsample):\n",
    "            count += 1\n",
    "            logits, _, concept_ids, node_type_ids, edge_index, edge_type = model(*input_data, detail=True)\n",
    "            print(qids, labels)\n",
    "            print(concept_ids)\n",
    "            predictions = logits.argmax(1) #[bsize, ]\n",
    "            preds_ranked = (-logits).argsort(1) #[bsize, n_choices]\n",
    "            for i, (qid, label, pred, _preds_ranked, cids, ntype, edges, etype) in enumerate(zip(qids, labels, predictions, preds_ranked, concept_ids, node_type_ids, edge_index, edge_type)):\n",
    "                acc = int(pred.item()==label.item())\n",
    "                print ('{},{}'.format(qid, chr(ord('A') + pred.item())), file=f_preds)\n",
    "                f_preds.flush()\n",
    "                total_acc.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['train-00000', 'train-00001'],\n",
       " tensor([2, 0], device='cuda:0'),\n",
       " tensor([[[   2,   43, 2900,  ...,    0,    0,    0],\n",
       "          [   2,   43, 2900,  ...,    0,    0,    0],\n",
       "          [   2,   43, 2900,  ...,    0,    0,    0],\n",
       "          [   2,   43, 2900,  ...,    0,    0,    0]],\n",
       " \n",
       "         [[   2,   43,   25,  ...,    0,    0,    0],\n",
       "          [   2,   43,   25,  ...,    0,    0,    0],\n",
       "          [   2,   43,   25,  ...,    0,    0,    0],\n",
       "          [   2,   43,   25,  ...,    0,    0,    0]]], device='cuda:0'),\n",
       " tensor([[[1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0]]], device='cuda:0'),\n",
       " tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0'),\n",
       " tensor([[[ True, False, False,  ...,  True,  True,  True],\n",
       "          [ True, False, False,  ...,  True,  True,  True],\n",
       "          [ True, False, False,  ...,  True,  True,  True],\n",
       "          [ True, False, False,  ...,  True,  True,  True]],\n",
       " \n",
       "         [[ True, False, False,  ...,  True,  True,  True],\n",
       "          [ True, False, False,  ...,  True,  True,  True],\n",
       "          [ True, False, False,  ...,  True,  True,  True],\n",
       "          [ True, False, False,  ...,  True,  True,  True]]], device='cuda:0'),\n",
       " tensor([[[   0,  553,  821,  ...,    1,    1,    1],\n",
       "          [   0,  553,  821,  ...,    1,    1,    1],\n",
       "          [   0,  553,  821,  ...,    1,    1,    1],\n",
       "          [   0,  553,  821,  ...,    1,    1,    1]],\n",
       " \n",
       "         [[   0,   57, 2089,  ...,    1,    1,    1],\n",
       "          [   0,   57, 2089,  ...,    1,    1,    1],\n",
       "          [   0,   57, 2089,  ...,    1,    1,    1],\n",
       "          [   0,   57, 2089,  ...,    1,    1,    1]]], device='cuda:0'),\n",
       " tensor([[[3, 0, 0,  ..., 2, 2, 2],\n",
       "          [3, 0, 0,  ..., 2, 2, 2],\n",
       "          [3, 0, 0,  ..., 2, 2, 2],\n",
       "          [3, 0, 0,  ..., 2, 2, 2]],\n",
       " \n",
       "         [[3, 0, 0,  ..., 2, 2, 2],\n",
       "          [3, 0, 0,  ..., 2, 2, 2],\n",
       "          [3, 0, 0,  ..., 2, 2, 2],\n",
       "          [3, 0, 0,  ..., 2, 2, 2]]], device='cuda:0'),\n",
       " tensor([[[[0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           ...,\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.]],\n",
       " \n",
       "          [[0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           ...,\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.]],\n",
       " \n",
       "          [[0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           ...,\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.]],\n",
       " \n",
       "          [[0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           ...,\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.]]],\n",
       " \n",
       " \n",
       "         [[[0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           ...,\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.]],\n",
       " \n",
       "          [[0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           ...,\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.]],\n",
       " \n",
       "          [[0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           ...,\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.]],\n",
       " \n",
       "          [[0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           ...,\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.]]]], device='cuda:0'),\n",
       " tensor([[50, 54, 52, 50],\n",
       "         [17, 21, 18, 18]], device='cuda:0'),\n",
       " [[tensor([[ 5,  6,  7,  8,  9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24,\n",
       "            25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42,\n",
       "            43, 45, 46, 47, 48,  9, 11, 13, 14, 15, 16, 21, 22, 23, 24, 25, 26, 29,\n",
       "            30, 31, 33, 35, 37, 38, 40, 41, 42, 43, 45, 46, 47, 48,  8,  8, 17, 33,\n",
       "            37, 10, 19, 12, 12, 12, 17, 18, 18, 19, 19, 27, 27, 27, 28, 28, 32, 32,\n",
       "            32, 34, 34, 39, 39, 49, 49,  0,  0,  0,  0,  0,  0,  0,  0, 20, 44, 44,\n",
       "            20,  3,  2, 20,  2,  2,  3,  2, 20, 20, 20,  2,  2,  2,  2,  3,  2, 20,\n",
       "            20,  3,  3,  2, 20, 20, 20,  3,  2, 20,  2, 20,  2,  2,  2,  2,  2,  3,\n",
       "             2,  2,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
       "             7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7, 36, 10,  1,  1,  1,  1, 27,\n",
       "             6,  7, 46,  7,  6,  7,  6,  7,  6,  7, 46,  6,  7,  6,  7, 46,  6,  7,\n",
       "             6,  7,  6,  7,  1,  2,  3,  4,  5,  6,  7,  8],\n",
       "           [20, 44, 44, 20,  3,  2, 20,  2,  2,  3,  2, 20, 20, 20,  2,  2,  2,  2,\n",
       "             3,  2, 20, 20,  3,  3,  2, 20, 20, 20,  3,  2, 20,  2, 20,  2,  2,  2,\n",
       "             2,  2,  3,  2,  2,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
       "             7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7, 36, 10,  1,  1,\n",
       "             1,  1, 27,  6,  7, 46,  7,  6,  7,  6,  7,  6,  7, 46,  6,  7,  6,  7,\n",
       "            46,  6,  7,  6,  7,  6,  7,  1,  2,  3,  4,  5,  6,  7,  8,  5,  6,  7,\n",
       "             8,  9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27,\n",
       "            28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 45, 46,\n",
       "            47, 48,  9, 11, 13, 14, 15, 16, 21, 22, 23, 24, 25, 26, 29, 30, 31, 33,\n",
       "            35, 37, 38, 40, 41, 42, 43, 45, 46, 47, 48,  8,  8, 17, 33, 37, 10, 19,\n",
       "            12, 12, 12, 17, 18, 18, 19, 19, 27, 27, 27, 28, 28, 32, 32, 32, 34, 34,\n",
       "            39, 39, 49, 49,  0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0'),\n",
       "   tensor([[ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22,\n",
       "            24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42,\n",
       "            43, 44, 45, 46, 47, 49, 50, 51, 52,  6,  8,  8,  8, 11, 12, 14, 14, 15,\n",
       "            16, 17, 18, 18, 20, 21, 24, 25, 26, 27, 27, 28, 29, 31, 32, 33, 34, 36,\n",
       "            38, 39, 39, 41, 43, 44, 44, 45, 47, 49, 50, 50, 51, 52,  9, 13,  8, 19,\n",
       "            36, 39, 40, 22,  8,  8, 14, 14, 14, 14, 19, 20, 20, 22, 22, 30, 30, 30,\n",
       "            31, 31, 35, 35, 35, 37, 37, 37, 42, 42, 53, 53,  0,  0,  0,  0,  0,  0,\n",
       "             0,  0, 23, 48, 48, 23, 48, 48,  3,  2, 48, 23,  2,  2,  3,  2, 23, 23,\n",
       "             3, 23,  2,  2,  2,  2,  3,  2, 23, 23,  3,  3,  2, 23, 23, 23,  3, 23,\n",
       "             2, 23,  2,  2,  2,  3,  2,  2,  3,  2,  2, 10,  9, 10, 13,  7,  7,  9,\n",
       "            10,  7,  7,  7,  7,  9, 10,  9,  7,  7,  7,  7, 13,  7,  7, 10,  7,  7,\n",
       "             7,  7,  7,  7, 13,  7,  7,  7, 13,  7,  7,  7,  7, 13,  7,  7,  6,  6,\n",
       "            40,  1,  1,  1,  1, 30, 21, 46,  6,  7, 13, 50,  7,  6,  7,  6,  7,  6,\n",
       "             7, 50,  6,  7,  6,  7, 50,  6,  7, 13,  6,  7,  6,  7,  1,  2,  3,  4,\n",
       "             5,  6,  7,  8],\n",
       "           [23, 48, 48, 23, 48, 48,  3,  2, 48, 23,  2,  2,  3,  2, 23, 23,  3, 23,\n",
       "             2,  2,  2,  2,  3,  2, 23, 23,  3,  3,  2, 23, 23, 23,  3, 23,  2, 23,\n",
       "             2,  2,  2,  3,  2,  2,  3,  2,  2, 10,  9, 10, 13,  7,  7,  9, 10,  7,\n",
       "             7,  7,  7,  9, 10,  9,  7,  7,  7,  7, 13,  7,  7, 10,  7,  7,  7,  7,\n",
       "             7,  7, 13,  7,  7,  7, 13,  7,  7,  7,  7, 13,  7,  7,  6,  6, 40,  1,\n",
       "             1,  1,  1, 30, 21, 46,  6,  7, 13, 50,  7,  6,  7,  6,  7,  6,  7, 50,\n",
       "             6,  7,  6,  7, 50,  6,  7, 13,  6,  7,  6,  7,  1,  2,  3,  4,  5,  6,\n",
       "             7,  8,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n",
       "            21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39,\n",
       "            41, 42, 43, 44, 45, 46, 47, 49, 50, 51, 52,  6,  8,  8,  8, 11, 12, 14,\n",
       "            14, 15, 16, 17, 18, 18, 20, 21, 24, 25, 26, 27, 27, 28, 29, 31, 32, 33,\n",
       "            34, 36, 38, 39, 39, 41, 43, 44, 44, 45, 47, 49, 50, 50, 51, 52,  9, 13,\n",
       "             8, 19, 36, 39, 40, 22,  8,  8, 14, 14, 14, 14, 19, 20, 20, 22, 22, 30,\n",
       "            30, 30, 31, 31, 35, 35, 35, 37, 37, 37, 42, 42, 53, 53,  0,  0,  0,  0,\n",
       "             0,  0,  0,  0]], device='cuda:0'),\n",
       "   tensor([[ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 22, 23,\n",
       "            24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41,\n",
       "            42, 43, 44, 45, 47, 48, 49, 50,  9, 10, 12, 13, 14, 15, 20, 22, 23, 24,\n",
       "            25, 26, 26, 29, 30, 32, 34, 36, 39, 40, 42, 42, 43, 43, 44, 45, 47, 48,\n",
       "            49, 50, 50, 50, 16, 34, 39, 18,  8,  8,  8,  8, 11, 11, 11, 16, 16, 17,\n",
       "            17, 18, 18, 27, 27, 27, 28, 28, 33, 33, 33, 35, 35, 41, 41, 51, 51,  0,\n",
       "             0,  0,  0,  0,  0,  0,  0, 19, 46, 46, 19,  3,  2, 19,  2,  2,  3,  2,\n",
       "            19, 19, 19,  2,  3,  2,  2,  2,  3,  2, 19, 19,  3,  3,  3,  2, 19, 19,\n",
       "            19,  3,  3,  3, 19,  2, 19,  2,  2,  2,  2,  2,  3,  2,  2,  7,  7,  7,\n",
       "             7,  7,  7,  7,  7,  7,  7,  7,  7, 31,  7,  7,  7,  7,  7,  7,  7,  7,\n",
       "            21,  7, 31,  7,  7,  7,  7,  7,  7, 31, 38,  1,  1,  1, 27, 21, 31, 37,\n",
       "            38,  6,  7, 48,  7, 31,  6,  7,  6,  7,  6,  7, 48,  6,  7,  6,  7, 48,\n",
       "             6,  7,  6,  7,  6,  7,  1,  2,  3,  4,  5,  6,  7,  8],\n",
       "           [19, 46, 46, 19,  3,  2, 19,  2,  2,  3,  2, 19, 19, 19,  2,  3,  2,  2,\n",
       "             2,  3,  2, 19, 19,  3,  3,  3,  2, 19, 19, 19,  3,  3,  3, 19,  2, 19,\n",
       "             2,  2,  2,  2,  2,  3,  2,  2,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
       "             7,  7, 31,  7,  7,  7,  7,  7,  7,  7,  7, 21,  7, 31,  7,  7,  7,  7,\n",
       "             7,  7, 31, 38,  1,  1,  1, 27, 21, 31, 37, 38,  6,  7, 48,  7, 31,  6,\n",
       "             7,  6,  7,  6,  7, 48,  6,  7,  6,  7, 48,  6,  7,  6,  7,  6,  7,  1,\n",
       "             2,  3,  4,  5,  6,  7,  8,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n",
       "            16, 17, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "            35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 47, 48, 49, 50,  9, 10, 12,\n",
       "            13, 14, 15, 20, 22, 23, 24, 25, 26, 26, 29, 30, 32, 34, 36, 39, 40, 42,\n",
       "            42, 43, 43, 44, 45, 47, 48, 49, 50, 50, 50, 16, 34, 39, 18,  8,  8,  8,\n",
       "             8, 11, 11, 11, 16, 16, 17, 17, 18, 18, 27, 27, 27, 28, 28, 33, 33, 33,\n",
       "            35, 35, 41, 41, 51, 51,  0,  0,  0,  0,  0,  0,  0,  0]],\n",
       "          device='cuda:0'),\n",
       "   tensor([[ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 22, 23,\n",
       "            24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41,\n",
       "            42, 43, 45, 46, 47, 48,  9, 10, 12, 13, 14, 15, 20, 21, 22, 23, 24, 25,\n",
       "            28, 29, 30, 32, 34, 36, 37, 39, 40, 41, 43, 45, 46, 47, 48, 48,  8, 16,\n",
       "            32, 36, 18,  8,  8, 11, 11, 11, 16, 17, 17, 18, 18, 26, 26, 26, 27, 27,\n",
       "            31, 31, 31, 33, 33, 38, 38, 49, 49,  0,  0,  0,  0,  0,  0,  0,  0, 19,\n",
       "            44, 44, 19,  3,  2, 19,  2,  2,  3,  2, 19, 19, 19,  2,  2,  2,  2,  3,\n",
       "             2, 19, 19,  3,  3,  2, 19, 19, 19,  3,  3, 19,  2, 19,  2,  2,  2,  3,\n",
       "             2,  2,  3,  2,  2,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
       "             7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7, 35,  1,  1,  1,\n",
       "             1, 26, 35, 42,  6,  7, 46,  7,  6,  7,  6,  7,  6,  7, 46,  6,  7,  6,\n",
       "             7, 46,  6,  7,  6,  7,  6,  7,  1,  2,  3,  4,  5,  6,  7,  8],\n",
       "           [19, 44, 44, 19,  3,  2, 19,  2,  2,  3,  2, 19, 19, 19,  2,  2,  2,  2,\n",
       "             3,  2, 19, 19,  3,  3,  2, 19, 19, 19,  3,  3, 19,  2, 19,  2,  2,  2,\n",
       "             3,  2,  2,  3,  2,  2,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
       "             7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7, 35,  1,  1,\n",
       "             1,  1, 26, 35, 42,  6,  7, 46,  7,  6,  7,  6,  7,  6,  7, 46,  6,  7,\n",
       "             6,  7, 46,  6,  7,  6,  7,  6,  7,  1,  2,  3,  4,  5,  6,  7,  8,  5,\n",
       "             6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 22, 23, 24,\n",
       "            25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42,\n",
       "            43, 45, 46, 47, 48,  9, 10, 12, 13, 14, 15, 20, 21, 22, 23, 24, 25, 28,\n",
       "            29, 30, 32, 34, 36, 37, 39, 40, 41, 43, 45, 46, 47, 48, 48,  8, 16, 32,\n",
       "            36, 18,  8,  8, 11, 11, 11, 16, 17, 17, 18, 18, 26, 26, 26, 27, 27, 31,\n",
       "            31, 31, 33, 33, 38, 38, 49, 49,  0,  0,  0,  0,  0,  0,  0,  0]],\n",
       "          device='cuda:0')],\n",
       "  [tensor([[ 1,  4,  5,  8,  9, 12, 16,  3,  3,  8,  9, 10, 10, 11, 11, 12, 13, 15,\n",
       "            15, 16,  3,  0,  0,  0,  0,  0,  0,  0, 14, 14, 14,  2, 14,  2,  2,  4,\n",
       "             9,  1,  1,  1,  3,  1,  3,  1,  1,  1,  4,  1, 13,  1,  2,  3,  4,  5,\n",
       "             6,  7],\n",
       "           [14, 14, 14,  2, 14,  2,  2,  4,  9,  1,  1,  1,  3,  1,  3,  1,  1,  1,\n",
       "             4,  1, 13,  1,  2,  3,  4,  5,  6,  7,  1,  4,  5,  8,  9, 12, 16,  3,\n",
       "             3,  8,  9, 10, 10, 11, 11, 12, 13, 15, 15, 16,  3,  0,  0,  0,  0,  0,\n",
       "             0,  0]], device='cuda:0'),\n",
       "   tensor([[ 1,  3,  4,  5,  7,  9, 10, 15, 20,  3,  3,  7,  8,  8,  9, 10, 11, 11,\n",
       "            12, 12, 13, 15, 16, 18, 18, 19, 19, 20, 13,  3,  0,  0,  0,  0,  0,  0,\n",
       "             0, 17, 14, 17, 17, 14,  2, 17,  2,  2,  4, 10,  1,  1,  7,  1,  1,  1,\n",
       "             3,  1,  3,  1,  1,  1,  1,  4,  1,  7,  1,  7, 16,  1,  2,  3,  4,  5,\n",
       "             6,  7],\n",
       "           [17, 14, 17, 17, 14,  2, 17,  2,  2,  4, 10,  1,  1,  7,  1,  1,  1,  3,\n",
       "             1,  3,  1,  1,  1,  1,  4,  1,  7,  1,  7, 16,  1,  2,  3,  4,  5,  6,\n",
       "             7,  1,  3,  4,  5,  7,  9, 10, 15, 20,  3,  3,  7,  8,  8,  9, 10, 11,\n",
       "            11, 12, 12, 13, 15, 16, 18, 18, 19, 19, 20, 13,  3,  0,  0,  0,  0,  0,\n",
       "             0,  0]], device='cuda:0'),\n",
       "   tensor([[ 1,  4,  5,  7,  8,  9, 13, 17,  3,  3,  8,  9, 10, 10, 11, 11, 12, 13,\n",
       "            14, 16, 16, 17,  3, 12,  0,  0,  0,  0,  0,  0,  0, 15, 15, 15,  2,  2,\n",
       "            15,  2,  2,  4,  9,  1,  1,  1,  3,  1,  3,  1,  1,  1,  1,  4,  1, 14,\n",
       "             7,  1,  2,  3,  4,  5,  6,  7],\n",
       "           [15, 15, 15,  2,  2, 15,  2,  2,  4,  9,  1,  1,  1,  3,  1,  3,  1,  1,\n",
       "             1,  1,  4,  1, 14,  7,  1,  2,  3,  4,  5,  6,  7,  1,  4,  5,  7,  8,\n",
       "             9, 13, 17,  3,  3,  8,  9, 10, 10, 11, 11, 12, 13, 14, 16, 16, 17,  3,\n",
       "            12,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0'),\n",
       "   tensor([[ 1,  4,  5,  7,  8,  9, 12, 17,  3,  3,  7,  7,  8,  9, 10, 10, 11, 11,\n",
       "            12, 13, 14, 16, 16, 17,  3,  0,  0,  0,  0,  0,  0,  0, 15, 15, 15, 15,\n",
       "             2, 15,  2,  2,  4,  9,  1, 13,  1,  1,  1,  3,  1,  3,  1,  1,  1,  1,\n",
       "             4,  1, 14,  1,  2,  3,  4,  5,  6,  7],\n",
       "           [15, 15, 15, 15,  2, 15,  2,  2,  4,  9,  1, 13,  1,  1,  1,  3,  1,  3,\n",
       "             1,  1,  1,  1,  4,  1, 14,  1,  2,  3,  4,  5,  6,  7,  1,  4,  5,  7,\n",
       "             8,  9, 12, 17,  3,  3,  7,  7,  8,  9, 10, 10, 11, 11, 12, 13, 14, 16,\n",
       "            16, 17,  3,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')]],\n",
       " [[tensor([ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "            2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "            2,  2,  2,  2,  2,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "            4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  6, 10, 10, 10,\n",
       "           10, 11, 14, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "           16, 16, 16, 16, 16, 16, 16,  0,  0,  0,  0,  0,  0,  0,  1, 19, 19, 19,\n",
       "           19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "           19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "           19, 19, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "           21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 23, 27, 27, 27, 27, 28, 31,\n",
       "           33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
       "           33, 33, 33, 33, 17, 17, 17, 17, 17, 17, 17, 18], device='cuda:0'),\n",
       "   tensor([ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "            2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "            2,  2,  2,  2,  2,  2,  2,  2,  2,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "            4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "            4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  5,  5, 10, 10,\n",
       "           10, 10, 11, 14, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "           16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,  0,  0,  0,  0,  0,  0,\n",
       "            0,  1, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "           19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "           19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 21, 21, 21, 21, 21, 21, 21,\n",
       "           21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "           21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 22, 22,\n",
       "           27, 27, 27, 27, 28, 31, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
       "           33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 17, 17, 17, 17,\n",
       "           17, 17, 17, 18], device='cuda:0'),\n",
       "   tensor([ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "            2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "            2,  2,  2,  2,  2,  2,  2,  2,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "            4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "            4,  4,  4,  4, 10, 10, 10, 14, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "           16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,  0,\n",
       "            0,  0,  0,  0,  0,  0,  1, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "           19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "           19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 21, 21, 21,\n",
       "           21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "           21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 27, 27, 27, 31, 33, 33, 33,\n",
       "           33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
       "           33, 33, 33, 33, 33, 33, 17, 17, 17, 17, 17, 17, 17, 18],\n",
       "          device='cuda:0'),\n",
       "   tensor([ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "            2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "            2,  2,  2,  2,  2,  2,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "            4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4, 10, 10,\n",
       "           10, 10, 14, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "           16, 16, 16, 16, 16, 16, 16, 16, 16,  0,  0,  0,  0,  0,  0,  0,  1, 19,\n",
       "           19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "           19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "           19, 19, 19, 19, 19, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "           21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 27, 27, 27,\n",
       "           27, 31, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
       "           33, 33, 33, 33, 33, 33, 33, 33, 17, 17, 17, 17, 17, 17, 17, 18],\n",
       "          device='cuda:0')],\n",
       "  [tensor([ 2,  2,  2,  2,  2,  2,  2,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "            4,  4,  6,  0,  0,  0,  0,  0,  0,  1, 19, 19, 19, 19, 19, 19, 19, 21,\n",
       "           21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 23, 17, 17, 17, 17, 17,\n",
       "           17, 18], device='cuda:0'),\n",
       "   tensor([ 2,  2,  2,  2,  2,  2,  2,  2,  2,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "            4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  5,  6,  0,  0,  0,  0,  0,  0,\n",
       "            1, 19, 19, 19, 19, 19, 19, 19, 19, 19, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "           21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 22, 23, 17, 17, 17, 17, 17,\n",
       "           17, 18], device='cuda:0'),\n",
       "   tensor([ 2,  2,  2,  2,  2,  2,  2,  2,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "            4,  4,  4,  4,  6,  6,  0,  0,  0,  0,  0,  0,  1, 19, 19, 19, 19, 19,\n",
       "           19, 19, 19, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 23,\n",
       "           23, 17, 17, 17, 17, 17, 17, 18], device='cuda:0'),\n",
       "   tensor([ 2,  2,  2,  2,  2,  2,  2,  2,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "            4,  4,  4,  4,  4,  4,  6,  0,  0,  0,  0,  0,  0,  1, 19, 19, 19, 19,\n",
       "           19, 19, 19, 19, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "           21, 21, 23, 17, 17, 17, 17, 17, 17, 18], device='cuda:0')]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "haystack-py310",
   "language": "python",
   "name": "haystack-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
